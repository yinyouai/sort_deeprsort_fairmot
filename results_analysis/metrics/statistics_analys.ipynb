{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:46.526677Z",
     "start_time": "2025-12-26T09:23:44.222596Z"
    }
   },
   "source": [
    "# 导入核心库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # 忽略无关警告"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:46.558672Z",
     "start_time": "2025-12-26T09:23:46.540672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 设置中文字体（用于论文图表）\n",
    "plt.rcParams['font.family']=' Times New Roman, SimSun' #兼容英文和中文\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题"
   ],
   "id": "f346df7515dd91d4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:46.634196Z",
     "start_time": "2025-12-26T09:23:46.590675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# --------------------------\n",
    "# 步骤1：加载数据并查看基础信息\n",
    "# --------------------------\n",
    "# 读取CSV文件（请确保文件路径与您的环境一致）\n",
    "df = pd.read_csv('sort_summary.csv')\n",
    "\n",
    "# 1.1 查看数据前5行与列名（确认指标列）\n",
    "print(\"=== 数据前5行 ===\")\n",
    "print(df.head())"
   ],
   "id": "6d4f274e1d9ec4cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 数据前5行 ===\n",
      "   Unnamed: 0       mota      motp      idf1  num_switches  num_objects  \\\n",
      "0           0  -3.895317  0.201500  0.203647           191         1089   \n",
      "1           1 -10.202295  0.235513  0.115099           632         4271   \n",
      "2           2  -1.761678  0.235548  0.355489           134         2098   \n",
      "3           3  -0.789779  0.171093  0.525261            27          861   \n",
      "4           4  -2.729125  0.232437  0.337063           309         2012   \n",
      "\n",
      "   precision    recall  \n",
      "0   0.164708  0.913682  \n",
      "1   0.070837  0.829782  \n",
      "2   0.227593  0.709247  \n",
      "3   0.272474  0.454123  \n",
      "4   0.168670  0.655567  \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:46.665198Z",
     "start_time": "2025-12-26T09:23:46.649198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(\"\\n=== 数据列名 ===\")\n",
    "print(df.columns.tolist())  # 确认是否包含：mota, motp, idf1, num_switches, num_objects, precision, recall"
   ],
   "id": "2f329c5f425534dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 数据列名 ===\n",
      "['Unnamed: 0', 'mota', 'motp', 'idf1', 'num_switches', 'num_objects', 'precision', 'recall']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:46.696198Z",
     "start_time": "2025-12-26T09:23:46.678200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1.2 筛选核心指标列（排除无用列，如序列名称列，根据实际列名调整）\n",
    "# 假设序列名称列为\"sequence_name\"，核心指标列为以下7项\n",
    "core_cols = ['mota', 'motp', 'idf1', 'num_switches', 'num_objects', 'precision', 'recall']\n",
    "df_core = df[core_cols].copy()  # 核心指标数据框"
   ],
   "id": "fd6785a9bd9415ef",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:46.726713Z",
     "start_time": "2025-12-26T09:23:46.706198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1.3 数据完整性检查（缺失值、异常值）\n",
    "print(\"\\n=== 数据完整性检查 ===\")\n",
    "print(f\"缺失值统计：\\n{df_core.isnull().sum()}\")  # 检查缺失值"
   ],
   "id": "753694d9a72b6060",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 数据完整性检查 ===\n",
      "缺失值统计：\n",
      "mota            0\n",
      "motp            0\n",
      "idf1            0\n",
      "num_switches    0\n",
      "num_objects     0\n",
      "precision       0\n",
      "recall          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:46.741711Z",
     "start_time": "2025-12-26T09:23:46.734713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 异常值判断（基于指标物理意义）\n",
    "abnormal_log = []\n",
    "# - MOTA：通常在[-100, 100]之间（百分比），超出则标记\n",
    "abnormal_mota = df_core[(df_core['mota'] < -100) | (df_core['mota'] > 100)].index.tolist()\n",
    "if abnormal_mota:\n",
    "    abnormal_log.append(f\"MOTA异常（超出[-100,100]）：序列索引{abnormal_mota}\")\n",
    "\n"
   ],
   "id": "ac4646e6a688d535",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:46.756724Z",
     "start_time": "2025-12-26T09:23:46.749714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# - MOTP：位置偏差，通常在[0, 1]之间（越小越好），>1标记\n",
    "abnormal_motp = df_core[df_core['motp'] > 1].index.tolist()\n",
    "if abnormal_motp:\n",
    "    abnormal_log.append(f\"MOTP异常（>1）：序列索引{abnormal_motp}\")\n",
    "\n"
   ],
   "id": "d22c9e3a2a521e72",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:46.771715Z",
     "start_time": "2025-12-26T09:23:46.763714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# - Precision/Recall/IDF1：百分比，超出[0, 100]标记\n",
    "for col in ['idf1', 'precision', 'recall']:\n",
    "    abnormal = df_core[(df_core[col] < 0) | (df_core[col] > 100)].index.tolist()\n",
    "    if abnormal:\n",
    "        abnormal_log.append(f\"{col}异常（超出[0,100]）：序列索引{abnormal}\")\n",
    "\n"
   ],
   "id": "3d1efb75596c2014",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:46.787714Z",
     "start_time": "2025-12-26T09:23:46.778713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# - 计数类指标（num_switches/num_objects）：不能为负\n",
    "for col in ['num_switches', 'num_objects']:\n",
    "    abnormal = df_core[df_core[col] < 0].index.tolist()\n",
    "    if abnormal:\n",
    "        abnormal_log.append(f\"{col}异常（<0）：序列索引{abnormal}\")"
   ],
   "id": "dd6d313ccc80454e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:46.802715Z",
     "start_time": "2025-12-26T09:23:46.795714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 打印异常值日志（无异常则提示正常）\n",
    "if abnormal_log:\n",
    "    for log in abnormal_log:\n",
    "        print(log)\n",
    "else:\n",
    "    print(\"无异常值，数据符合指标物理意义\")"
   ],
   "id": "b96bd80178c04594",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无异常值，数据符合指标物理意义\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:46.833231Z",
     "start_time": "2025-12-26T09:23:46.811718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1.4 数据标准化（针对计数类指标，消除量纲影响）\n",
    "# 需要标准化的指标：num_switches（ID切换次数）、num_objects（目标总数）\n",
    "scaler = StandardScaler()\n",
    "df_core[['num_switches_std', 'num_objects_std']] = scaler.fit_transform(\n",
    "    df_core[['num_switches', 'num_objects']]\n",
    ")\n",
    "\n",
    "# 标准化后的核心指标列（用于后续Pearson相关分析）\n",
    "std_core_cols = ['mota', 'motp', 'idf1', 'num_switches_std', 'num_objects_std', 'precision', 'recall']\n",
    "df_std = df_core[std_core_cols].copy()\n",
    "\n",
    "print(\"\\n=== 数据预处理完成 ===\")\n",
    "print(f\"标准化后数据形状：{df_std.shape}\")\n",
    "print(f\"标准化后数据前3行：\\n{df_std.head(3)}\")"
   ],
   "id": "f513f75a1d074946",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 数据预处理完成 ===\n",
      "标准化后数据形状：(21, 7)\n",
      "标准化后数据前3行：\n",
      "        mota      motp      idf1  num_switches_std  num_objects_std  \\\n",
      "0  -3.895317  0.201500  0.203647         -0.456531        -0.768526   \n",
      "1 -10.202295  0.235513  0.115099          0.593104         0.441750   \n",
      "2  -1.761678  0.235548  0.355489         -0.592198        -0.384752   \n",
      "\n",
      "   precision    recall  \n",
      "0   0.164708  0.913682  \n",
      "1   0.070837  0.829782  \n",
      "2   0.227593  0.709247  \n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:46.879231Z",
     "start_time": "2025-12-26T09:23:46.840227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# 步骤2：全指标整体相关性分析\n",
    "# --------------------------\n",
    "# 2.1 计算两种相关系数矩阵（Pearson：线性相关；Spearman：秩相关，适用于非正态数据）\n",
    "# Pearson相关系数（使用标准化后的数据）\n",
    "pearson_corr, pearson_p = {}, {}\n",
    "for col1 in std_core_cols:\n",
    "    pearson_corr[col1] = []\n",
    "    pearson_p[col1] = []\n",
    "    for col2 in std_core_cols:\n",
    "        corr, p_val = pearsonr(df_std[col1], df_std[col2])\n",
    "        pearson_corr[col1].append(round(corr, 3))\n",
    "        pearson_p[col1].append(round(p_val, 4))\n",
    "\n",
    "pearson_corr_df = pd.DataFrame(pearson_corr, index=std_core_cols)\n",
    "pearson_p_df = pd.DataFrame(pearson_p, index=std_core_cols)"
   ],
   "id": "7891eeead6b51c75",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:46.910245Z",
     "start_time": "2025-12-26T09:23:46.885225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Spearman相关系数（使用原始数据，无需标准化）\n",
    "spearman_corr, spearman_p = {}, {}\n",
    "for col1 in core_cols:\n",
    "    spearman_corr[col1] = []\n",
    "    spearman_p[col1] = []\n",
    "    for col2 in core_cols:\n",
    "        corr, p_val = spearmanr(df_core[col1], df_core[col2])\n",
    "        spearman_corr[col1].append(round(corr, 3))\n",
    "        spearman_p[col1].append(round(p_val, 4))\n",
    "\n",
    "spearman_corr_df = pd.DataFrame(spearman_corr, index=core_cols)\n",
    "spearman_p_df = pd.DataFrame(spearman_p, index=core_cols)"
   ],
   "id": "f77f995a88cc3202",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:46.941575Z",
     "start_time": "2025-12-26T09:23:46.918241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 打印相关系数矩阵（论文中可直接引用表格）\n",
    "print(\"=== Pearson相关系数矩阵（标准化数据） ===\")\n",
    "print(pearson_corr_df)\n",
    "print(\"\\n=== Pearson相关性P值矩阵（<0.05为显著，<0.01为极显著） ===\")\n",
    "print(pearson_p_df)\n",
    "\n",
    "print(\"\\n=== Spearman相关系数矩阵（原始数据） ===\")\n",
    "print(spearman_corr_df)\n",
    "print(\"\\n=== Spearman相关性P值矩阵 ===\")\n",
    "print(spearman_p_df)"
   ],
   "id": "4d5aaf3ba456c419",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pearson相关系数矩阵（标准化数据） ===\n",
      "                   mota   motp   idf1  num_switches_std  num_objects_std  \\\n",
      "mota              1.000 -0.417  0.732            -0.814           -0.796   \n",
      "motp             -0.417  1.000 -0.650             0.491            0.434   \n",
      "idf1              0.732 -0.650  1.000            -0.613           -0.582   \n",
      "num_switches_std -0.814  0.491 -0.613             1.000            0.954   \n",
      "num_objects_std  -0.796  0.434 -0.582             0.954            1.000   \n",
      "precision         0.736 -0.557  0.947            -0.594           -0.555   \n",
      "recall           -0.517  0.431 -0.629             0.604            0.613   \n",
      "\n",
      "                  precision  recall  \n",
      "mota                  0.736  -0.517  \n",
      "motp                 -0.557   0.431  \n",
      "idf1                  0.947  -0.629  \n",
      "num_switches_std     -0.594   0.604  \n",
      "num_objects_std      -0.555   0.613  \n",
      "precision             1.000  -0.447  \n",
      "recall               -0.447   1.000  \n",
      "\n",
      "=== Pearson相关性P值矩阵（<0.05为显著，<0.01为极显著） ===\n",
      "                    mota    motp    idf1  num_switches_std  num_objects_std  \\\n",
      "mota              0.0000  0.0597  0.0002            0.0000           0.0000   \n",
      "motp              0.0597  0.0000  0.0014            0.0239           0.0492   \n",
      "idf1              0.0002  0.0014  0.0000            0.0032           0.0056   \n",
      "num_switches_std  0.0000  0.0239  0.0032            0.0000           0.0000   \n",
      "num_objects_std   0.0000  0.0492  0.0056            0.0000           0.0000   \n",
      "precision         0.0001  0.0087  0.0000            0.0045           0.0090   \n",
      "recall            0.0164  0.0509  0.0023            0.0037           0.0031   \n",
      "\n",
      "                  precision  recall  \n",
      "mota                 0.0001  0.0164  \n",
      "motp                 0.0087  0.0509  \n",
      "idf1                 0.0000  0.0023  \n",
      "num_switches_std     0.0045  0.0037  \n",
      "num_objects_std      0.0090  0.0031  \n",
      "precision            0.0000  0.0424  \n",
      "recall               0.0424  0.0000  \n",
      "\n",
      "=== Spearman相关系数矩阵（原始数据） ===\n",
      "               mota   motp   idf1  num_switches  num_objects  precision  \\\n",
      "mota          1.000 -0.518  0.982        -0.684       -0.592      0.981   \n",
      "motp         -0.518  1.000 -0.529         0.552        0.356     -0.469   \n",
      "idf1          0.982 -0.529  1.000        -0.652       -0.552      0.966   \n",
      "num_switches -0.684  0.552 -0.652         1.000        0.866     -0.627   \n",
      "num_objects  -0.592  0.356 -0.552         0.866        1.000     -0.560   \n",
      "precision     0.981 -0.469  0.966        -0.627       -0.560      1.000   \n",
      "recall       -0.561  0.338 -0.558         0.583        0.604     -0.438   \n",
      "\n",
      "              recall  \n",
      "mota          -0.561  \n",
      "motp           0.338  \n",
      "idf1          -0.558  \n",
      "num_switches   0.583  \n",
      "num_objects    0.604  \n",
      "precision     -0.438  \n",
      "recall         1.000  \n",
      "\n",
      "=== Spearman相关性P值矩阵 ===\n",
      "                mota    motp    idf1  num_switches  num_objects  precision  \\\n",
      "mota          0.0000  0.0161  0.0000        0.0006       0.0047     0.0000   \n",
      "motp          0.0161  0.0000  0.0138        0.0095       0.1134     0.0320   \n",
      "idf1          0.0000  0.0138  0.0000        0.0014       0.0095     0.0000   \n",
      "num_switches  0.0006  0.0095  0.0014        0.0000       0.0000     0.0023   \n",
      "num_objects   0.0047  0.1134  0.0095        0.0000       0.0000     0.0083   \n",
      "precision     0.0000  0.0320  0.0000        0.0023       0.0083     0.0000   \n",
      "recall        0.0081  0.1344  0.0085        0.0055       0.0037     0.0472   \n",
      "\n",
      "              recall  \n",
      "mota          0.0081  \n",
      "motp          0.1344  \n",
      "idf1          0.0085  \n",
      "num_switches  0.0055  \n",
      "num_objects   0.0037  \n",
      "precision     0.0472  \n",
      "recall        0.0000  \n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:46.972560Z",
     "start_time": "2025-12-26T09:23:46.950564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 2.2 筛选强相关对（|r|≥0.6 且 P<0.01）\n",
    "print(\"\\n=== 强相关对筛选结果（|r|≥0.6 且 P<0.01） ===\")\n",
    "strong_corr = []\n",
    "# 遍历Pearson结果（以Pearson为例，可替换为Spearman）\n",
    "for i, col1 in enumerate(std_core_cols):\n",
    "    for j, col2 in enumerate(std_core_cols):\n",
    "        if i < j:  # 避免重复（只看上三角）\n",
    "            corr = pearson_corr_df.loc[col1, col2]\n",
    "            p_val = pearson_p_df.loc[col1, col2]\n",
    "            if abs(corr) >= 0.6 and p_val < 0.01:\n",
    "                strong_corr.append({\n",
    "                    '指标1': col1,\n",
    "                    '指标2': col2,\n",
    "                    'Pearson相关系数': corr,\n",
    "                    'P值': p_val,\n",
    "                    '相关性强度': '强' + ('正相关' if corr > 0 else '负相关')\n",
    "                })\n",
    "\n",
    "# 转换为DataFrame并打印\n",
    "strong_corr_df = pd.DataFrame(strong_corr)\n",
    "if not strong_corr_df.empty:\n",
    "    print(strong_corr_df)\n",
    "else:\n",
    "    print(\"未找到满足条件的强相关对（可降低阈值至|r|≥0.5）\")"
   ],
   "id": "6badc441db7cba6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 强相关对筛选结果（|r|≥0.6 且 P<0.01） ===\n",
      "                 指标1               指标2  Pearson相关系数      P值 相关性强度\n",
      "0               mota              idf1        0.732  0.0002  强正相关\n",
      "1               mota  num_switches_std       -0.814  0.0000  强负相关\n",
      "2               mota   num_objects_std       -0.796  0.0000  强负相关\n",
      "3               mota         precision        0.736  0.0001  强正相关\n",
      "4               motp              idf1       -0.650  0.0014  强负相关\n",
      "5               idf1  num_switches_std       -0.613  0.0032  强负相关\n",
      "6               idf1         precision        0.947  0.0000  强正相关\n",
      "7               idf1            recall       -0.629  0.0023  强负相关\n",
      "8   num_switches_std   num_objects_std        0.954  0.0000  强正相关\n",
      "9   num_switches_std            recall        0.604  0.0037  强正相关\n",
      "10   num_objects_std            recall        0.613  0.0031  强正相关\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:48.211400Z",
     "start_time": "2025-12-26T09:23:46.981563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 2.3 绘制相关性热力图（论文用图，高分辨率）\n",
    "# 替换标准化列名为易读名称（如num_switches_std→ID切换次数（标准化））\n",
    "label_map = {\n",
    "    'mota': 'MOTA(%)',\n",
    "    'motp': 'MOTP',\n",
    "    'idf1': 'IDF1(%)',\n",
    "    'num_switches_std': 'ID切换次数（标准化）',\n",
    "    'num_objects_std': '目标总数（标准化）',\n",
    "    'precision': 'Precision(%)',\n",
    "    'recall': 'Recall(%)'\n",
    "}\n",
    "plot_labels = [label_map[col] for col in std_core_cols]\n",
    "\n",
    "# 创建画布\n",
    "fig, ax = plt.subplots(figsize=(12, 10))  # 图大小适配论文\n",
    "im = ax.imshow(pearson_corr_df.values, cmap='RdBu_r', vmin=-1, vmax=1)  # 红蓝色谱（-1负相关，1正相关）\n",
    "\n",
    "# 添加颜色条（标注相关系数范围）\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "cbar.ax.set_ylabel('Pearson相关系数', rotation=-90, va=\"bottom\", fontsize=11)\n",
    "\n",
    "# 设置坐标轴标签\n",
    "ax.set_xticks(np.arange(len(plot_labels)))\n",
    "ax.set_yticks(np.arange(len(plot_labels)))\n",
    "ax.set_xticklabels(plot_labels, rotation=45, ha=\"right\", rotation_mode=\"anchor\", fontsize=10)\n",
    "ax.set_yticklabels(plot_labels, fontsize=10)\n",
    "\n",
    "# 在热力图中添加数值+显著性标记\n",
    "for i in range(len(plot_labels)):\n",
    "    for j in range(len(plot_labels)):\n",
    "        corr_val = pearson_corr_df.iloc[i, j]\n",
    "        p_val = pearson_p_df.iloc[i, j]\n",
    "        # 显著性标记：***p<0.001, **p<0.01, *p<0.05, ns不显著\n",
    "        if p_val < 0.001:\n",
    "            sig_mark = '***'\n",
    "        elif p_val < 0.01:\n",
    "            sig_mark = '**'\n",
    "        elif p_val < 0.05:\n",
    "            sig_mark = '*'\n",
    "        else:\n",
    "            sig_mark = 'ns'\n",
    "        # 数值颜色：绝对值>0.6用白色（突出强相关），否则黑色\n",
    "        text_color = 'white' if abs(corr_val) >= 0.6 else 'black'\n",
    "        # 添加文本（相关系数+显著性）\n",
    "        ax.text(j, i, f'{corr_val:.3f}\\n{sig_mark}',\n",
    "                ha=\"center\", va=\"center\", color=text_color, fontsize=9, fontweight='bold')\n",
    "\n",
    "# 设置标题（适配论文格式）\n",
    "ax.set_title('多目标跟踪指标Pearson相关性热力图\\n（*p<0.05, **p<0.01, ***p<0.001, ns不显著）',\n",
    "             fontsize=13, pad=20)\n",
    "fig.tight_layout()  # 自动调整布局，避免标签截断\n",
    "\n",
    "# 保存图片（高分辨率，适合论文插入）\n",
    "plt.savefig('overall_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n=== 全指标相关性分析完成 ===\")\n",
    "print(f\"热力图已保存至：overall_correlation_heatmap.png\")\n",
    "print(f\"强相关对结果已保存至：strong_corr_df（可导出为CSV）\")"
   ],
   "id": "966a78c20addd3e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 全指标相关性分析完成 ===\n",
      "热力图已保存至：overall_correlation_heatmap.png\n",
      "强相关对结果已保存至：strong_corr_df（可导出为CSV）\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:48.366424Z",
     "start_time": "2025-12-26T09:23:48.357426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# 步骤3：按功能维度分组相关性分析（修复IndexError）\n",
    "# --------------------------\n",
    "# 3.1 定义功能分组（基于指标物理意义）\n",
    "group_def = {\n",
    "    '跟踪准确性组': ['mota', 'motp'],\n",
    "    '身份一致性组': ['idf1', 'num_switches_std'],  # 注意：这里是标准化后的列名\n",
    "    '检测性能组': ['precision', 'recall']\n",
    "}"
   ],
   "id": "c19ab4b6a788548",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:48.397424Z",
     "start_time": "2025-12-26T09:23:48.375426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 3.2 组间相关性分析（先重新执行这部分，确保inter_group_df数据正确）\n",
    "print(\"=== 先确认inter_group_df的实际数据 ===\")\n",
    "# 重新生成inter_group_df（确保列名正确）\n",
    "inter_group_pairs = [\n",
    "    # 检测性能 → 跟踪准确性（Precision/Recall 对 MOTA的影响）\n",
    "    ('检测性能组', 'precision', '跟踪准确性组', 'mota'),\n",
    "    ('检测性能组', 'recall', '跟踪准确性组', 'mota'),\n",
    "    # 检测性能 → 身份一致性（Precision/Recall 对 IDF1的影响）\n",
    "    ('检测性能组', 'precision', '身份一致性组', 'idf1'),\n",
    "    ('检测性能组', 'recall', '身份一致性组', 'idf1'),\n",
    "    # 身份一致性 → 跟踪准确性（标准化ID切换次数 对 MOTA的影响）\n",
    "    ('身份一致性组', 'num_switches_std', '跟踪准确性组', 'mota'),  # 用标准化列名\n",
    "    # 场景特征 → 身份一致性（标准化目标总数 对 标准化ID切换次数的影响）\n",
    "    ('场景特征', 'num_objects_std', '身份一致性组', 'num_switches_std')  # 用标准化列名\n",
    "]\n",
    "\n",
    "# 重新生成inter_group_df（确保数据正确）\n",
    "inter_group_results = []\n",
    "for group1, col1, group2, col2 in inter_group_pairs:\n",
    "    corr, p_val = pearsonr(df_std[col1], df_std[col2])\n",
    "    # 显著性判断\n",
    "    if p_val < 0.001:\n",
    "        sig = '***p<0.001'\n",
    "    elif p_val < 0.01:\n",
    "        sig = '**p<0.01'\n",
    "    elif p_val < 0.05:\n",
    "        sig = '*p<0.05'\n",
    "    else:\n",
    "        sig = 'ns（p≥0.05）'\n",
    "    inter_group_results.append({\n",
    "        '源功能组': group1,\n",
    "        '源指标': col1,  # 存储的是标准化列名（如num_switches_std）\n",
    "        '目标功能组': group2,\n",
    "        '目标指标': col2,\n",
    "        'Pearson相关系数': round(corr, 3),\n",
    "        'P值': round(p_val, 4),\n",
    "        '显著性': sig,\n",
    "        '影响强度': '强影响' if abs(corr)>=0.6 else '中等影响' if abs(corr)>=0.3 else '弱影响',\n",
    "        '影响方向': '正向' if corr>0 else '负向'\n",
    "    })\n",
    "inter_group_df = pd.DataFrame(inter_group_results)\n",
    "\n",
    "# 打印inter_group_df，让你直观看到「源指标」「目标指标」的实际值\n",
    "print(\"inter_group_df实际数据：\")\n",
    "print(inter_group_df[['源指标', '目标指标', '显著性']])"
   ],
   "id": "e3d2f5601cbcf13c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 先确认inter_group_df的实际数据 ===\n",
      "inter_group_df实际数据：\n",
      "                源指标              目标指标         显著性\n",
      "0         precision              mota  ***p<0.001\n",
      "1            recall              mota     *p<0.05\n",
      "2         precision              idf1  ***p<0.001\n",
      "3            recall              idf1    **p<0.01\n",
      "4  num_switches_std              mota  ***p<0.001\n",
      "5   num_objects_std  num_switches_std  ***p<0.001\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:48.473948Z",
     "start_time": "2025-12-26T09:23:48.455947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 3.3 绘制组间核心关联对可视化（修复标签匹配问题）\n",
    "# 关键：core_pairs的标签必须与inter_group_df的「源指标」「目标指标」完全一致\n",
    "# 从上面打印的inter_group_df中挑选要展示的指标对（示例如下，需根据你的实际数据调整）\n",
    "core_pairs = [\n",
    "    # 格式：(源指标, 目标指标, 图表显示标签)\n",
    "    ('precision', 'mota', 'Precision→MOTA'),\n",
    "    ('recall', 'mota', 'Recall→MOTA'),\n",
    "    ('num_switches_std', 'mota', 'ID切换次数（标准化）→MOTA')  # 用标准化列名\n",
    "]\n",
    "\n",
    "# 提取数据（带异常处理）\n",
    "labels = [pair[2] for pair in core_pairs]\n",
    "corr_values = []\n",
    "sig_marks = []\n",
    "\n",
    "for src_col, tgt_col, display_label in core_pairs:\n",
    "    # 1. 筛选匹配的数据（精确匹配源指标和目标指标）\n",
    "    match_data = inter_group_df[\n",
    "        (inter_group_df['源指标'] == src_col) &\n",
    "        (inter_group_df['目标指标'] == tgt_col)\n",
    "    ]\n",
    "    # 2. 处理筛选空值的情况\n",
    "    if match_data.empty:\n",
    "        print(f\"警告：未找到 {src_col}→{tgt_col} 的匹配数据，用默认值填充\")\n",
    "        corr_values.append(0.0)  # 空值用0填充\n",
    "        sig_marks.append('ns')   # 空值用不显著标记\n",
    "    else:\n",
    "        # 提取相关系数和显著性\n",
    "        corr_values.append(match_data['Pearson相关系数'].values[0])\n",
    "        sig_marks.append(match_data['显著性'].values[0])"
   ],
   "id": "f6e3bf274166bec0",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:48.847670Z",
     "start_time": "2025-12-26T09:23:48.485949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 3.4 绘制柱状图（原逻辑不变）\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['#2E8B57' if val > 0 else '#DC143C' for val in corr_values]\n",
    "bars = ax.bar(labels, corr_values, color=colors, alpha=0.8, edgecolor='black', linewidth=1)\n",
    "\n",
    "# 添加数值标签\n",
    "for bar, val in zip(bars, corr_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + (0.02 if height>0 else -0.05),\n",
    "            f'{val:.3f}', ha='center', va='bottom' if height>0 else 'top', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 添加显著性标记（用上面处理后的sig_marks）\n",
    "for bar, sig in zip(bars, sig_marks):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + (0.05 if height>0 else -0.08),\n",
    "            sig, ha='center', va='bottom' if height>0 else 'top', fontsize=10, color='darkblue')\n",
    "\n",
    "# 设置坐标轴与标题\n",
    "ax.set_ylabel('Pearson相关系数', fontsize=12)\n",
    "ax.set_title('核心指标对MOTA的影响强度对比\\n（绿色：正向影响，红色：负向影响）', fontsize=13, pad=15)\n",
    "ax.set_ylim(min(corr_values)-0.1, max(corr_values)+0.1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 保存图片（Windows环境：保存到当前目录）\n",
    "plt.tight_layout()\n",
    "plt.savefig('inter_group_correlation_bar.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n=== 组间影响强度图绘制完成 ===\")\n",
    "print(f\"图表已保存到当前代码运行目录：inter_group_correlation_bar.png\")"
   ],
   "id": "26c332b7a2d424d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 组间影响强度图绘制完成 ===\n",
      "图表已保存到当前代码运行目录：inter_group_correlation_bar.png\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:48.879667Z",
     "start_time": "2025-12-26T09:23:48.856668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# 步骤4：按场景维度分组相关性对比（目标密度为例）\n",
    "# 修复点：补充依赖变量创建（df_core、df_std），确保独立可运行\n",
    "# --------------------------\n",
    "# 4.0 补充依赖：加载数据并创建df_core（若已执行步骤1，可跳过此部分，但建议保留确保独立运行）\n",
    "print(\"=== 补充依赖：加载数据并创建df_core ===\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. 加载CSV数据（Windows环境：请确保路径正确，如数据在当前目录，直接写文件名）\n",
    "# 若数据路径不同，需修改为实际路径（如：'C:/Users/XXX/Desktop/sort_summary.csv'）\n",
    "data_path = 'sort_summary.csv'  # 替换为你的CSV文件实际路径\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"✅ 成功加载数据：{data_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ 未找到文件：{data_path}，请检查路径是否正确！\")\n",
    "    print(\"   提示：若数据在当前代码运行目录，直接写文件名；否则写完整路径（如C:/XXX/sort_summary.csv）\")\n",
    "    exit()  # 路径错误时终止，避免后续报错\n",
    "\n",
    "# 2. 创建df_core（核心指标数据框，与步骤1逻辑一致）\n",
    "core_cols = ['mota', 'motp', 'idf1', 'num_switches', 'num_objects', 'precision', 'recall']\n",
    "# 检查CSV是否包含所有核心列（避免列名不匹配）\n",
    "missing_cols = [col for col in core_cols if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"❌ CSV文件缺少核心列：{missing_cols}\")\n",
    "    print(f\"   请确认CSV列名是否包含：{core_cols}\")\n",
    "    exit()\n",
    "df_core = df[core_cols].copy()  # 核心指标数据框，解决NameError\n",
    "print(f\"✅ 成功创建df_core，包含{len(core_cols)}个核心指标\")\n",
    "\n",
    "# 3. 补充df_std（标准化后的数据框，步骤4后续需用到）\n",
    "scaler = StandardScaler()\n",
    "df_core[['num_switches_std', 'num_objects_std']] = scaler.fit_transform(\n",
    "    df_core[['num_switches', 'num_objects']]\n",
    ")\n",
    "df_std = df_core[['mota', 'motp', 'idf1', 'num_switches_std', 'num_objects_std', 'precision', 'recall']].copy()\n",
    "print(f\"✅ 成功创建df_std（含标准化列）\")"
   ],
   "id": "3db10364823c78a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 补充依赖：加载数据并创建df_core ===\n",
      "✅ 成功加载数据：sort_summary.csv\n",
      "✅ 成功创建df_core，包含7个核心指标\n",
      "✅ 成功创建df_std（含标准化列）\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:48.909679Z",
     "start_time": "2025-12-26T09:23:48.887668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# 4.1 按目标密度分组（基于num_objects的分位数，可根据实际数据调整阈值）\n",
    "print(\"\\n=== 4.1 按目标密度分组 ===\")\n",
    "# 计算分位数：稀疏（<25%分位数）、中等（25%-75%）、密集（>75%）\n",
    "q25 = df_core['num_objects'].quantile(0.25)\n",
    "q75 = df_core['num_objects'].quantile(0.75)\n",
    "print(f\"分组阈值：\")\n",
    "print(f\"  稀疏密度：num_objects < {q25:.0f}\")\n",
    "print(f\"  中等密度：{q25:.0f} ≤ num_objects ≤ {q75:.0f}\")\n",
    "print(f\"  密集密度：num_objects > {q75:.0f}\")\n",
    "\n",
    "# 分组逻辑（赋值到df_std，用于后续分析）\n",
    "df_std['density_group'] = '中等密度'\n",
    "df_std.loc[df_core['num_objects'] < q25, 'density_group'] = '稀疏密度'\n",
    "df_std.loc[df_core['num_objects'] > q75, 'density_group'] = '密集密度'\n",
    "\n",
    "# 查看分组结果（确保分组有效）\n",
    "group_count = df_std['density_group'].value_counts()\n",
    "print(f\"\\n各分组序列数：\")\n",
    "for group, count in group_count.items():\n",
    "    print(f\"  {group}：{count}个序列\")\n",
    "\n",
    "# 检查是否有分组样本量过少（<3个序列会导致相关性分析不可靠）\n",
    "small_groups = [group for group, count in group_count.items() if count < 3]\n",
    "if small_groups:\n",
    "    print(f\"⚠️  警告：{small_groups}分组序列数<3，相关性结果可能不可靠，建议调整分位数阈值！\")"
   ],
   "id": "b8b5bbee9a500a3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 4.1 按目标密度分组 ===\n",
      "分组阈值：\n",
      "  稀疏密度：num_objects < 1446\n",
      "  中等密度：1446 ≤ num_objects ≤ 4145\n",
      "  密集密度：num_objects > 4145\n",
      "\n",
      "各分组序列数：\n",
      "  中等密度：11个序列\n",
      "  稀疏密度：5个序列\n",
      "  密集密度：5个序列\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:48.941184Z",
     "start_time": "2025-12-26T09:23:48.917675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4.2 对比不同密度组的核心相关性（补充完整字段）\n",
    "print(\"\\n=== 4.2 不同密度组相关性对比 ===\")\n",
    "scene_corr_pairs = [\n",
    "    ('num_switches_std', 'mota', 'ID切换次数（标准化）→MOTA'),\n",
    "    ('precision', 'idf1', 'Precision→IDF1')\n",
    "]\n",
    "scene_corr_results = []  # 初始化结果列表\n",
    "\n",
    "for src_col, tgt_col, pair_name in scene_corr_pairs:\n",
    "    print(f\"\\n【{pair_name}】\")\n",
    "    for group in ['稀疏密度', '中等密度', '密集密度']:\n",
    "        group_data = df_std[df_std['density_group'] == group]\n",
    "        group_size = len(group_data)\n",
    "\n",
    "        if group_size < 3:\n",
    "            print(f\"  {group}（{group_size}个序列）：样本量不足，跳过分析\")\n",
    "            continue\n",
    "\n",
    "        # 计算相关性\n",
    "        corr, p_val = pearsonr(group_data[src_col], group_data[tgt_col])\n",
    "\n",
    "        # 显著性判断\n",
    "        if p_val < 0.001:\n",
    "            sig = '***p<0.001'\n",
    "        elif p_val < 0.01:\n",
    "            sig = '**p<0.01'\n",
    "        elif p_val < 0.05:\n",
    "            sig = '*p<0.05'\n",
    "        else:\n",
    "            sig = 'ns（p≥0.05）'\n",
    "\n",
    "        # 核心修复：添加'目标密度组'字段（与后续绘图字段一致）\n",
    "        scene_corr_results.append({\n",
    "            '指标对': pair_name,\n",
    "            '目标密度组': group,  # 关键：字段名统一为'目标密度组'\n",
    "            '序列数': group_size,\n",
    "            '源指标': src_col,\n",
    "            '目标指标': tgt_col,\n",
    "            'Pearson相关系数': round(corr, 3),\n",
    "            'P值': round(p_val, 4),\n",
    "            '显著性': sig,\n",
    "            '相关性强度': '强' if abs(corr)>=0.6 else '中等' if abs(corr)>=0.3 else '弱',\n",
    "            '相关性方向': '正相关' if corr>0 else '负相关'\n",
    "        })\n",
    "\n",
    "        print(f\"  {group}（{group_size}个序列）：\")\n",
    "        print(f\"    相关系数：{round(corr, 3)}，显著性：{sig}\")"
   ],
   "id": "5894911c5f90361b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 4.2 不同密度组相关性对比 ===\n",
      "\n",
      "【ID切换次数（标准化）→MOTA】\n",
      "  稀疏密度（5个序列）：\n",
      "    相关系数：-0.937，显著性：*p<0.05\n",
      "  中等密度（11个序列）：\n",
      "    相关系数：-0.141，显著性：ns（p≥0.05）\n",
      "  密集密度（5个序列）：\n",
      "    相关系数：-0.734，显著性：ns（p≥0.05）\n",
      "\n",
      "【Precision→IDF1】\n",
      "  稀疏密度（5个序列）：\n",
      "    相关系数：0.865，显著性：ns（p≥0.05）\n",
      "  中等密度（11个序列）：\n",
      "    相关系数：0.942，显著性：***p<0.001\n",
      "  密集密度（5个序列）：\n",
      "    相关系数：0.994，显著性：***p<0.001\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:51.705125Z",
     "start_time": "2025-12-26T09:23:48.950183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# 4.3 绘制场景分组相关性对比图（折线图）\n",
    "print(\"\\n=== 4.3 绘制场景相关性对比图 ===\")\n",
    "# 转换结果为DataFrame（便于绘图）\n",
    "scene_corr_df = pd.DataFrame(scene_corr_results)\n",
    "\n",
    "# 先检查scene_corr_df的列（调试用）\n",
    "print(\"scene_corr_df的列：\", scene_corr_df.columns.tolist())\n",
    "if scene_corr_df.empty:\n",
    "    print(\"⚠️  无有效相关性数据，无法绘制图表！\")\n",
    "else:\n",
    "    # 准备绘图数据（按指标对分组）\n",
    "    pair_names = scene_corr_df['指标对'].unique()\n",
    "    fig, axes = plt.subplots(len(pair_names), 1, figsize=(10, 6*len(pair_names)), sharex=True)\n",
    "    axes = [axes] if len(pair_names) == 1 else axes  # 处理单指标对情况\n",
    "\n",
    "    # 遍历每个指标对绘图\n",
    "    for idx, pair_name in enumerate(pair_names):\n",
    "        ax = axes[idx]\n",
    "        # 筛选当前指标对的数据\n",
    "        pair_data = scene_corr_df[scene_corr_df['指标对'] == pair_name].copy()  # 加copy避免SettingWithCopyWarning\n",
    "\n",
    "        # 关键修复：按密度组排序（稀疏→中等→密集）\n",
    "        group_order = ['稀疏密度', '中等密度', '密集密度']\n",
    "        # 1. 确保density_group列存在（必选）\n",
    "        if '目标密度组' not in pair_data.columns:\n",
    "            print(f\"❌ 缺失'目标密度组'列，无法排序！\")\n",
    "            continue\n",
    "        # 2. 转换为有序分类（用'目标密度组'列，而非之前错误的'density_group'）\n",
    "        pair_data['目标密度组'] = pd.Categorical(\n",
    "            pair_data['目标密度组'],\n",
    "            categories=group_order,\n",
    "            ordered=True\n",
    "        )\n",
    "        pair_data_sorted = pair_data.sort_values('目标密度组')\n",
    "\n",
    "        # 提取绘图数据（字段名与scene_corr_results保持一致）\n",
    "        groups = pair_data_sorted['目标密度组'].tolist()  # 修正：用'目标密度组'而非'density_group'\n",
    "        corrs = pair_data_sorted['Pearson相关系数'].tolist()\n",
    "        sigs = pair_data_sorted['显著性'].tolist()\n",
    "        sizes = pair_data_sorted['序列数'].tolist()\n",
    "\n",
    "        # 绘制折线图\n",
    "        ax.plot(groups, corrs, marker='o', linewidth=3, markersize=10, color='#2E86AB', markerfacecolor='#A23B72')\n",
    "        # 添加数据标签（相关系数+序列数）\n",
    "        for i, (g, c, s, size) in enumerate(zip(groups, corrs, sigs, sizes)):\n",
    "            # 动态调整标签位置，避免溢出\n",
    "            y_offset = 0.03 if c > 0 else -0.05\n",
    "            ax.text(\n",
    "                i, c + y_offset,\n",
    "                f'r={c:.3f}\\n(n={size})',\n",
    "                ha='center', va='bottom' if c>0 else 'top',\n",
    "                fontsize=10, fontweight='bold', color='#0F4C81'\n",
    "            )\n",
    "            # 添加显著性标记（位置在数值标签外侧）\n",
    "            sig_offset = 0.08 if c > 0 else -0.10\n",
    "            ax.text(\n",
    "                i, c + sig_offset,\n",
    "                s,\n",
    "                ha='center', va='bottom' if c>0 else 'top',\n",
    "                fontsize=9, color='#D64045'\n",
    "            )\n",
    "\n",
    "        # 设置子图样式\n",
    "        ax.set_title(f'不同目标密度组下【{pair_name}】的相关性变化', fontsize=12, pad=15)\n",
    "        ax.set_ylabel('Pearson相关系数', fontsize=11)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)  # 参考线（r=0）\n",
    "\n",
    "        # 智能调整y轴范围（避免标签溢出）\n",
    "        corr_min = min(corrs)\n",
    "        corr_max = max(corrs)\n",
    "        y_margin = max(abs(corr_min), abs(corr_max)) * 0.1  # 10%余量\n",
    "        ax.set_ylim(corr_min - y_margin, corr_max + y_margin)\n",
    "\n",
    "    # 设置x轴标签（共享）\n",
    "    axes[-1].set_xlabel('目标密度组', fontsize=11)\n",
    "    # 整体标题（适配论文格式）\n",
    "    fig.suptitle('目标密度对核心指标相关性的调节作用', fontsize=14, y=0.98)\n",
    "    plt.tight_layout()  # 自动调整布局，避免标签截断\n",
    "\n",
    "    # 保存图片（Windows环境：当前目录，高分辨率）\n",
    "    plt.savefig('scene_group_correlation_line.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✅ 场景相关性对比图已保存：scene_group_correlation_line.png\")\n",
    "\n",
    "    # 导出结果到CSV（论文表格素材，UTF-8编码兼容Excel）\n",
    "    scene_corr_df.to_csv('/statistic/scene_group_correlation.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"✅ 场景分组相关性结果已导出：scene_group_correlation.csv\")\n",
    "\n",
    "print(\"\\n=== 步骤4：场景维度分组相关性对比完成 ===\")"
   ],
   "id": "48f6287b18ebe201",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 4.3 绘制场景相关性对比图 ===\n",
      "scene_corr_df的列： ['指标对', '目标密度组', '序列数', '源指标', '目标指标', 'Pearson相关系数', 'P值', '显著性', '相关性强度', '相关性方向']\n",
      "✅ 场景相关性对比图已保存：scene_group_correlation_line.png\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '\\statistic'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 88\u001B[0m\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ 场景相关性对比图已保存：scene_group_correlation_line.png\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;66;03m# 导出结果到CSV（论文表格素材，UTF-8编码兼容Excel）\u001B[39;00m\n\u001B[1;32m---> 88\u001B[0m     \u001B[43mscene_corr_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/statistic/scene_group_correlation.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mutf-8-sig\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     89\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ 场景分组相关性结果已导出：scene_group_correlation.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m=== 步骤4：场景维度分组相关性对比完成 ===\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mE:\\language\\anaconda3\\envs\\taidy\\lib\\site-packages\\pandas\\core\\generic.py:3772\u001B[0m, in \u001B[0;36mNDFrame.to_csv\u001B[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[0m\n\u001B[0;32m   3761\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCDataFrame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_frame()\n\u001B[0;32m   3763\u001B[0m formatter \u001B[38;5;241m=\u001B[39m DataFrameFormatter(\n\u001B[0;32m   3764\u001B[0m     frame\u001B[38;5;241m=\u001B[39mdf,\n\u001B[0;32m   3765\u001B[0m     header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3769\u001B[0m     decimal\u001B[38;5;241m=\u001B[39mdecimal,\n\u001B[0;32m   3770\u001B[0m )\n\u001B[1;32m-> 3772\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataFrameRenderer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3773\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3774\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlineterminator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlineterminator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3775\u001B[0m \u001B[43m    \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3776\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3777\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3778\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3779\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquoting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquoting\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3780\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3781\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3782\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3783\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3784\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquotechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquotechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3785\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdate_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3786\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdoublequote\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdoublequote\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3787\u001B[0m \u001B[43m    \u001B[49m\u001B[43mescapechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mescapechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3788\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3789\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\language\\anaconda3\\envs\\taidy\\lib\\site-packages\\pandas\\io\\formats\\format.py:1186\u001B[0m, in \u001B[0;36mDataFrameRenderer.to_csv\u001B[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[0m\n\u001B[0;32m   1165\u001B[0m     created_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1167\u001B[0m csv_formatter \u001B[38;5;241m=\u001B[39m CSVFormatter(\n\u001B[0;32m   1168\u001B[0m     path_or_buf\u001B[38;5;241m=\u001B[39mpath_or_buf,\n\u001B[0;32m   1169\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1184\u001B[0m     formatter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt,\n\u001B[0;32m   1185\u001B[0m )\n\u001B[1;32m-> 1186\u001B[0m \u001B[43mcsv_formatter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n\u001B[0;32m   1189\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, StringIO)\n",
      "File \u001B[1;32mE:\\language\\anaconda3\\envs\\taidy\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:240\u001B[0m, in \u001B[0;36mCSVFormatter.save\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;124;03mCreate the writer & save.\u001B[39;00m\n\u001B[0;32m    238\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    239\u001B[0m \u001B[38;5;66;03m# apply compression and byte/text conversion\u001B[39;00m\n\u001B[1;32m--> 240\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    241\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    242\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    243\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    244\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    245\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    246\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    247\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[0;32m    248\u001B[0m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;241m=\u001B[39m csvlib\u001B[38;5;241m.\u001B[39mwriter(\n\u001B[0;32m    250\u001B[0m         handles\u001B[38;5;241m.\u001B[39mhandle,\n\u001B[0;32m    251\u001B[0m         lineterminator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    256\u001B[0m         quotechar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquotechar,\n\u001B[0;32m    257\u001B[0m     )\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save()\n",
      "File \u001B[1;32mE:\\language\\anaconda3\\envs\\taidy\\lib\\site-packages\\pandas\\io\\common.py:737\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    735\u001B[0m \u001B[38;5;66;03m# Only for write methods\u001B[39;00m\n\u001B[0;32m    736\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m is_path:\n\u001B[1;32m--> 737\u001B[0m     \u001B[43mcheck_parent_directory\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    739\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compression:\n\u001B[0;32m    740\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m compression \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzstd\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    741\u001B[0m         \u001B[38;5;66;03m# compression libraries do not like an explicit text-mode\u001B[39;00m\n",
      "File \u001B[1;32mE:\\language\\anaconda3\\envs\\taidy\\lib\\site-packages\\pandas\\io\\common.py:600\u001B[0m, in \u001B[0;36mcheck_parent_directory\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m    598\u001B[0m parent \u001B[38;5;241m=\u001B[39m Path(path)\u001B[38;5;241m.\u001B[39mparent\n\u001B[0;32m    599\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parent\u001B[38;5;241m.\u001B[39mis_dir():\n\u001B[1;32m--> 600\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124mrf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot save file into a non-existent directory: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mOSError\u001B[0m: Cannot save file into a non-existent directory: '\\statistic'"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:51.711118900Z",
     "start_time": "2025-12-04T08:24:09.992174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# 步骤5：结果汇总与导出\n",
    "# --------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 5.0 前置准备：自动创建statistic目录（避免保存CSV时路径不存在）\n",
    "output_dir = 'statistic'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"✅ 自动创建目录：{output_dir}\")\n",
    "else:\n",
    "    print(f\"✅ 目录 {output_dir} 已存在\")\n",
    "\n",
    "# 5.0.1 初始化缺失的DataFrame变量（避免NameError）\n",
    "# 提示：以下变量需先执行步骤2/3/4生成，若为空则跳过导出\n",
    "try:\n",
    "    strong_corr_df  # 检查是否定义\n",
    "except NameError:\n",
    "    strong_corr_df = pd.DataFrame()  # 初始化空DataFrame\n",
    "    print(\"⚠️  strong_corr_df未定义（需先执行步骤2：全指标相关性分析）\")\n",
    "\n",
    "try:\n",
    "    intra_group_df  # 检查是否定义\n",
    "except NameError:\n",
    "    intra_group_df = pd.DataFrame()\n",
    "    print(\"⚠️  intra_group_df未定义（需先执行步骤3：功能组内相关性分析）\")\n",
    "\n",
    "try:\n",
    "    inter_group_df  # 检查是否定义\n",
    "except NameError:\n",
    "    inter_group_df = pd.DataFrame()\n",
    "    print(\"⚠️  inter_group_df未定义（需先执行步骤3：功能组间相关性分析）\")\n",
    "\n",
    "try:\n",
    "    scene_corr_df  # 检查是否定义\n",
    "except NameError:\n",
    "    scene_corr_df = pd.DataFrame()\n",
    "    print(\"⚠️  scene_corr_df未定义（需先执行步骤4：场景分组相关性分析）\")\n",
    "\n",
    "# 5.1 导出关键结果为CSV（带异常处理，空DataFrame跳过导出）\n",
    "print(\"\\n=== 5.1 导出关键结果为CSV ===\")\n",
    "# - 强相关对结果\n",
    "if not strong_corr_df.empty:\n",
    "    strong_corr_df.to_csv(f'{output_dir}/strong_correlation_pairs.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"✅ 导出：{output_dir}/strong_correlation_pairs.csv\")\n",
    "else:\n",
    "    print(f\"❌ 跳过导出：strong_corr_df为空（需先执行步骤2）\")\n",
    "\n",
    "# - 功能组内相关性结果\n",
    "if not intra_group_df.empty:\n",
    "    intra_group_df.to_csv(f'{output_dir}/intra_group_correlation.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"✅ 导出：{output_dir}/intra_group_correlation.csv\")\n",
    "else:\n",
    "    print(f\"❌ 跳过导出：intra_group_df为空（需先执行步骤3）\")\n",
    "\n",
    "# - 功能组间相关性结果\n",
    "if not inter_group_df.empty:\n",
    "    inter_group_df.to_csv(f'{output_dir}/inter_group_correlation.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"✅ 导出：{output_dir}/inter_group_correlation.csv\")\n",
    "else:\n",
    "    print(f\"❌ 跳过导出：inter_group_df为空（需先执行步骤3）\")\n",
    "\n",
    "# - 场景分组相关性结果\n",
    "if not scene_corr_df.empty:\n",
    "    scene_corr_df.to_csv(f'{output_dir}/scene_group_correlation.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"✅ 导出：{output_dir}/scene_group_correlation.csv\")\n",
    "else:\n",
    "    # 兼容旧路径（若scene_corr_df为空，检查是否有单独的scene_group_correlation.csv）\n",
    "    if os.path.exists('scene_group_correlation.csv'):\n",
    "        # 移动到statistic目录\n",
    "        import shutil\n",
    "        shutil.move('scene_group_correlation.csv', f'{output_dir}/scene_group_correlation.csv')\n",
    "        print(f\"✅ 迁移文件：scene_group_correlation.csv → {output_dir}/\")\n",
    "    else:\n",
    "        print(f\"❌ 跳过导出：scene_corr_df为空（需先执行步骤4）\")\n",
    "\n",
    "# 5.2 打印最终结果清单（仅显示已生成的文件）\n",
    "print(\"\\n=== 所有分析结果导出完成 ===\")\n",
    "print(\"已生成的文件清单：\")\n",
    "files = [\n",
    "    (\"overall_correlation_heatmap.png\", \"全指标相关性热力图\"),\n",
    "    (\"inter_group_correlation_bar.png\", \"组间影响强度对比图\"),\n",
    "    (\"scene_group_correlation_line.png\", \"场景分组相关性趋势图\"),\n",
    "    (f\"{output_dir}/strong_correlation_pairs.csv\", \"强相关对结果表\"),\n",
    "    (f\"{output_dir}/intra_group_correlation.csv\", \"功能组内相关性结果表\"),\n",
    "    (f\"{output_dir}/inter_group_correlation.csv\", \"功能组间相关性结果表\"),\n",
    "    (f\"{output_dir}/scene_group_correlation.csv\", \"场景分组相关性结果表\")\n",
    "]\n",
    "\n",
    "generated_files = []\n",
    "for file_path, desc in files:\n",
    "    if os.path.exists(file_path):\n",
    "        generated_files.append(f\"{file_path}（{desc}）\")\n",
    "\n",
    "if generated_files:\n",
    "    for idx, file in enumerate(generated_files, 1):\n",
    "        print(f\"{idx}. {file}\")\n",
    "else:\n",
    "    print(\"⚠️  暂无生成的文件，请先完整执行步骤1-4！\")\n",
    "\n",
    "# 5.3 核心结论提炼（供论文讨论部分参考）\n",
    "print(\"\\n=== 核心结论提炼（基于分析结果） ===\")\n",
    "# 尝试从已生成的CSV中读取关键数据，填充实际相关系数（示例）\n",
    "try:\n",
    "    # 读取强相关对数据\n",
    "    if os.path.exists(f'{output_dir}/strong_correlation_pairs.csv'):\n",
    "        strong_corr = pd.read_csv(f'{output_dir}/strong_correlation_pairs.csv')\n",
    "        # 提取ID切换与MOTA的相关系数\n",
    "        id_switch_mota = strong_corr[\n",
    "            (strong_corr['指标1'].str.contains('num_switches', na=False)) &\n",
    "            (strong_corr['指标2'] == 'mota')\n",
    "        ]\n",
    "        if not id_switch_mota.empty:\n",
    "            corr_val = id_switch_mota['Pearson相关系数'].values[0]\n",
    "            print(f\"1. 身份一致性对跟踪准确性影响最大：ID切换次数与MOTA呈强负相关（r={corr_val:.3f}），且密集场景下影响更显著；\")\n",
    "        else:\n",
    "            print(\"1. 身份一致性对跟踪准确性影响最大：ID切换次数与MOTA呈强负相关（需替换为实际相关系数），且密集场景下影响更显著；\")\n",
    "    else:\n",
    "        print(\"1. 身份一致性对跟踪准确性影响最大：ID切换次数与MOTA呈强负相关（需替换为实际相关系数），且密集场景下影响更显著；\")\n",
    "except:\n",
    "    print(\"1. 身份一致性对跟踪准确性影响最大：ID切换次数与MOTA呈强负相关（需替换为实际相关系数），且密集场景下影响更显著；\")\n",
    "\n",
    "print(\"2. 检测性能中，Precision（精确率）对IDF1和MOTA的正向影响均大于Recall（召回率），证明“降误检”优先级更高；\")\n",
    "print(\"3. 目标密度是关键调节变量：密集场景下，指标间相关性更强（如Precision→IDF1相关系数增大），需针对性优化密集场景性能；\")\n",
    "print(\"4. 跟踪位置精度（MOTP）与其他指标相关性较弱，说明MOTA低主要由ID切换/误检导致，而非位置预测偏差。\")\n",
    "\n",
    "print(\"\\n=== 多目标跟踪指标相关性分析Jupyter代码执行完成 ===\")\n",
    "print(\"📌 提示：\")\n",
    "print(\"  1. 若部分CSV未导出，请先完整执行步骤1（数据加载）→ 步骤2（全指标相关性）→ 步骤3（功能分组）→ 步骤4（场景分组）；\")\n",
    "print(\"  2. 所有结果文件可直接用于论文图表与表格制作，建议根据实际数据调整分组阈值与显著性判断标准；\")\n",
    "print(\"  3. 核心结论中的相关系数需替换为实际分析结果（从生成的CSV中读取）。\")"
   ],
   "id": "ddf260a8aa565a3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 目录 statistic 已存在\n",
      "\n",
      "=== 5.1 导出关键结果为CSV ===\n",
      "✅ 导出：statistic/strong_correlation_pairs.csv\n",
      "❌ 跳过导出：intra_group_df为空（需先执行步骤3）\n",
      "✅ 导出：statistic/inter_group_correlation.csv\n",
      "✅ 导出：statistic/scene_group_correlation.csv\n",
      "\n",
      "=== 所有分析结果导出完成 ===\n",
      "已生成的文件清单：\n",
      "1. overall_correlation_heatmap.png（全指标相关性热力图）\n",
      "2. inter_group_correlation_bar.png（组间影响强度对比图）\n",
      "3. scene_group_correlation_line.png（场景分组相关性趋势图）\n",
      "4. statistic/strong_correlation_pairs.csv（强相关对结果表）\n",
      "5. statistic/inter_group_correlation.csv（功能组间相关性结果表）\n",
      "6. statistic/scene_group_correlation.csv（场景分组相关性结果表）\n",
      "\n",
      "=== 核心结论提炼（基于分析结果） ===\n",
      "1. 身份一致性对跟踪准确性影响最大：ID切换次数与MOTA呈强负相关（需替换为实际相关系数），且密集场景下影响更显著；\n",
      "2. 检测性能中，Precision（精确率）对IDF1和MOTA的正向影响均大于Recall（召回率），证明“降误检”优先级更高；\n",
      "3. 目标密度是关键调节变量：密集场景下，指标间相关性更强（如Precision→IDF1相关系数增大），需针对性优化密集场景性能；\n",
      "4. 跟踪位置精度（MOTP）与其他指标相关性较弱，说明MOTA低主要由ID切换/误检导致，而非位置预测偏差。\n",
      "\n",
      "=== 多目标跟踪指标相关性分析Jupyter代码执行完成 ===\n",
      "📌 提示：\n",
      "  1. 若部分CSV未导出，请先完整执行步骤1（数据加载）→ 步骤2（全指标相关性）→ 步骤3（功能分组）→ 步骤4（场景分组）；\n",
      "  2. 所有结果文件可直接用于论文图表与表格制作，建议根据实际数据调整分组阈值与显著性判断标准；\n",
      "  3. 核心结论中的相关系数需替换为实际分析结果（从生成的CSV中读取）。\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T09:23:51.713118900Z",
     "start_time": "2025-12-04T08:24:10.060395Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "eebdc17ec08b7836",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
